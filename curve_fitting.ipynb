{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afd6969d",
   "metadata": {},
   "source": [
    "# Curve Fitting via Gradient Descent\n",
    "This notebook walks through fitting a data to a model using **gradient descent**, breaking down the optimization problem step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3ce933-0036-4a84-9895-ad477e4eab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2390c1cf-fcd0-4ebe-b5cf-0809201e5df3",
   "metadata": {},
   "source": [
    "## 1. The principles of curve fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf672664-3536-477c-8052-4867bdcb2b50",
   "metadata": {},
   "source": [
    "Suppose we observe the flight of a cannon ball. It *should* be a parabola (a quadratic function), but our observations will have some noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ea8f04-e335-40ba-9b19-055bded6ad0f",
   "metadata": {},
   "source": [
    "Since the parameters $a$ and $b$ in the expression $ax^2 + bx$ will vary in our examples, we'll make $f$ a function of $x$ and of the parameters $a$ and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85325d45-f45b-4476-b84f-b381fd6a7866",
   "metadata": {},
   "source": [
    "Mathematically, the function will be $f(x; a, b) = ax^2 + bx$. This can be expressed in Python as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c15f09-feb7-4e76-8b55-dffce442e699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, params):\n",
    "    a, b = params\n",
    "    return a * x**2 + b * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05dc9f2-faee-4e63-a4d9-c77482ee6d70",
   "metadata": {},
   "source": [
    "Let's plot this function for $0 \\le x \\le 10$ where $a = -2$ and $b = 20$. That is, we're plotting $f(x, -2, 20)$ for $0 \\le x \\le 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d532bc8b-c735-478b-b3e6-c00b885763d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True parameters\n",
    "a_true, b_true = -2, 20\n",
    "params = (a_true, b_true)\n",
    "x = np.linspace(0, 10, 100)\n",
    "y_true = f(x, params)\n",
    "\n",
    "# Plot data\n",
    "plt.plot(x, y_true, 'r-', label='True function')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5744014-8860-4c6d-a1be-069916b7711f",
   "metadata": {},
   "source": [
    "We can simulate some *noisy* data by adding Gaussian (normally distributed) noise to the true function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ce7609-6062-4d46-adec-ecd1f60aea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_noisy = y_true + 2 * np.random.normal(size=x.size)\n",
    "\n",
    "# Plot data\n",
    "plt.scatter(x, y_noisy, label='Noisy data')\n",
    "plt.plot(x, y_true, 'r-', label='True function')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e010ffc9-ca5b-48d3-9dbc-a0f2eead36a0",
   "metadata": {},
   "source": [
    "## 2. Measuring the error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a6b951-6efe-4399-989c-26e39246ced4",
   "metadata": {},
   "source": [
    "What if we *don't* know the parameters $a$ and $b$? If we only have the data, how would we estimate the parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57f05b2-58d5-4703-ab7e-347eafadb6ec",
   "metadata": {},
   "source": [
    "Here is some real data from an observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13416fff-d53e-4b8d-ac86-627819c0ac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_real = np.array(\n",
    "      [ 2.36940577,  1.91256656,  3.29100004,  7.37237899,  8.18812265,\n",
    "       18.11820761, 14.32964108, 23.45234193, 23.62329705, 30.8287573 ,\n",
    "       26.80806497, 32.79398541, 35.06553443, 32.21923115, 39.24730387,\n",
    "       42.92070059, 37.44160446, 46.44210828, 47.98489458, 50.6389694 ,\n",
    "       53.82037364, 51.7647055 , 56.38908139, 56.26206115, 61.48547156,\n",
    "       59.12983318, 61.26146892, 61.38177205, 61.7767669 , 67.62397392,\n",
    "       69.1165745 , 68.00363628, 68.06124047, 66.05407344, 74.01905039,\n",
    "       72.48996972, 77.38943085, 74.95610992, 71.69048994, 76.73277233,\n",
    "       75.61839857, 75.18028164, 80.42250882, 78.57339426, 74.52704359,\n",
    "       74.43486386, 82.60185162, 80.63046288, 79.27165332, 83.2890151 ,\n",
    "       78.7393532 , 77.31638276, 76.05766149, 79.93963762, 85.7438112 ,\n",
    "       78.3924083 , 75.06743955, 73.37798833, 80.24202993, 77.18560786,\n",
    "       78.01280817, 78.06876329, 73.34550463, 72.32834926, 75.91838852,\n",
    "       76.06033239, 73.46894369, 70.89037719, 69.35203876, 69.2537387 ,\n",
    "       64.88669121, 61.9744727 , 66.14014532, 61.41478708, 62.08014846,\n",
    "       59.97290679, 55.71001107, 55.72145555, 55.60052885, 51.0761939 ,\n",
    "       48.45070483, 46.66118886, 45.4664427 , 49.08572602, 41.67735713,\n",
    "       42.42097927, 37.24459493, 33.91618618, 29.18840866, 27.44201465,\n",
    "       28.78153111, 27.63522533, 21.24672748, 18.50438048, 10.3768495 ,\n",
    "       11.26647595, 11.80014487,  5.91476614,  1.61126693,  1.5332323 ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd045965-30aa-49dd-abfc-2e7702a05f30",
   "metadata": {},
   "source": [
    "### Activity 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd54031-def0-4554-8d26-45b1298b980b",
   "metadata": {},
   "source": [
    "**Play** with the parameters `a_guess` and `b_guess` belows to try to find a good fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9841696-bbbb-4df9-b779-acac0182cac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guess parameters. Change these!!\n",
    "a_guess, b_guess = -1.5, 20\n",
    "\n",
    "y_guess = f(x, (a_guess, b_guess))\n",
    "plt.scatter(x, y_real, label='Real data')\n",
    "plt.plot(x, y_guess, 'r-', label='Estimated function')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01ce283",
   "metadata": {},
   "source": [
    "## 2. The Optimization Problem\n",
    "We want to find parameters $(a, b)$ that minimize the **sum of squared errors**:\n",
    "\n",
    "$$\n",
    "J(a, b) = \\frac{1}{2n} \\sum_{i=1}^n (y_i - f(x_i; a, b))^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34ba4f0-1c01-45c2-b8f0-e58d712f7e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(x, y, params):\n",
    "    n = x.size\n",
    "    residuals = y - f(x, params)\n",
    "    return (residuals**2).sum() / (2*n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af209723-171d-4118-8ef4-f917baa272c4",
   "metadata": {},
   "source": [
    "### Activity 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39356872-fb14-4dba-a6c6-81b38969ded7",
   "metadata": {},
   "source": [
    "Play with the values `a_guess` and `b_guess` below to try to minimise $J$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297e94b6-4479-40ee-b1af-6a3acbafb110",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_guess, b_guess = -2, 30\n",
    "\n",
    "J(x,y_real, (a_guess, b_guess))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba4941d",
   "metadata": {},
   "source": [
    "## 3. Gradient Descent\n",
    "Gradient descent is a general method for numerically finding the minimal value of a function of many variables.\n",
    "\n",
    "**Idea** if you're standing on a hill and want to find the bottom of the hill, walk in the steepest downward direction.\n",
    "\n",
    "It iteratively updates parameters in the direction that **reduces** the cost. The update rule for each parameter $\\theta_j$ is:\n",
    "\n",
    "$$\n",
    "\\theta_j \\leftarrow \\theta_j - \\alpha \\frac{\\partial J}{\\partial \\theta_j},\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\alpha$ is the **learning rate** (step size).\n",
    "- $\\frac{\\partial J}{\\partial \\theta_j}$ is the gradient of the cost with respect to $\\theta_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77aa4cee-7d1c-45d0-ba64-7e7472c54afd",
   "metadata": {},
   "source": [
    "For our model $f(x; a,b) = ax^2 + bx$, recall\n",
    "$$\n",
    "J(a, b) = \\frac{1}{2n} \\sum_{i=1}^n (y_i - ax_i^2 - bx_i)^2.\n",
    "$$\n",
    "\n",
    "So, writing $f_i = ax_i^2 + bx_i$, the partial derivatives are\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial a} = -\\frac{1}{n} \\sum (y_i - f_i) x_i^2,\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b} = -\\frac{1}{n} \\sum (y_i - f_i) x_i.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865317ae-de75-4e84-a7ce-12798a18604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contour_plot(func, size):\n",
    "    a_vals = np.linspace(-size, size, 100)   # adjust range as needed\n",
    "    b_vals = np.linspace(-size, size, 100)   # adjust range as needed\n",
    "    A, B = np.meshgrid(a_vals, b_vals)\n",
    "    \n",
    "    # 4) Compute the cost on the grid\n",
    "    Cost = np.empty_like(A)\n",
    "    for i in range(A.shape[0]):\n",
    "        for j in range(A.shape[1]):\n",
    "            Cost[i, j] = func(A[i, j], B[i, j])\n",
    "    \n",
    "    # 5) Plot the contour\n",
    "    plt.figure(figsize=(6,5))\n",
    "    CS = plt.contour(A, B, Cost, levels=15)\n",
    "    plt.clabel(CS, inline=True, fontsize=8)\n",
    "    plt.xlabel('a parameter')\n",
    "    plt.ylabel('b parameter')\n",
    "    plt.title('Contour Plot')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28f5dde-df48-440e-9a61-ac6b18ac49a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_plot(lambda a, b : J(x, y_real, (a, b)), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5623b0f",
   "metadata": {},
   "source": [
    "## 4. Implementing Gradient Descent\n",
    "Let's code gradient descent to fit the parameters. We'll track the cost over iterations and inspect convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c541860-ef68-43ca-a1ab-064ca9618b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x, y, params):\n",
    "    m = x.size\n",
    "    y_pred = f(x, params)\n",
    "    residuals = y - y_pred\n",
    "    # gradients\n",
    "    da = -np.sum(residuals * (x*x)) / m\n",
    "    db = -np.sum(residuals * x) / m\n",
    "    return np.array([da, db])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be0cfc6-2432-4a76-bb34-ad147e2794e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(func, params, h=1e-5):\n",
    "    \"\"\"\n",
    "    Numerically estimates the gradient of func at params using central differences.\n",
    "    \"\"\"\n",
    "    params = np.array(params, dtype=float)\n",
    "    grad = np.zeros_like(params)\n",
    "\n",
    "    for i in range(len(params)):\n",
    "        step = np.zeros_like(params)\n",
    "        step[i] = h\n",
    "        grad[i] = (func(params + step) - func(params - step)) / (2 * h)\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f926470-9058-4e82-a42a-84decd352184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contour_with_gradients(func, size=10):\n",
    "    a_vals = np.linspace(-size, size, 100)   # adjust range as needed\n",
    "    b_vals = np.linspace(-size, size, 100)   # adjust range as needed\n",
    "    A, B = np.meshgrid(a_vals, b_vals)\n",
    "    \n",
    "    # 4) Compute the cost on the grid\n",
    "    Cost = np.empty_like(A)\n",
    "    for i in range(A.shape[0]):\n",
    "        for j in range(A.shape[1]):\n",
    "            Cost[i, j] = func((A[i, j], B[i, j]))\n",
    "            \n",
    "    A_s = A[::step, ::step]\n",
    "    B_s = B[::step, ::step]\n",
    "    dA = np.zeros_like(A_s)\n",
    "    dB = np.zeros_like(B_s)\n",
    "    for i in range(A_s.shape[0]):\n",
    "        for j in range(A_s.shape[1]):\n",
    "            grad = numerical_gradient(func,(A_s[i,j], B_s[i,j]))\n",
    "            # we want arrows pointing downhill, so negate the gradient:\n",
    "            dA[i,j], dB[i,j] = -grad\n",
    "    \n",
    "    norm = np.linalg.norm(np.array((dA, dB)), axis=0)\n",
    "    dA = dA / norm\n",
    "    dB = dB / norm\n",
    "    \n",
    "    # 5) Plot contour + quiver\n",
    "    plt.figure(figsize=(6,5))\n",
    "    CS = plt.contour(A, B, Cost, levels=20, cmap = 'cividis')\n",
    "    plt.clabel(CS, inline=True, fontsize=8)\n",
    "    plt.quiver(A_s, B_s, dA, dB, \n",
    "               angles='xy', scale_units='xy', scale=20/size, width=0.003, color='red')\n",
    "    plt.xlabel('a parameter')\n",
    "    plt.ylabel('b parameter')\n",
    "    plt.title('Contours with Gradient Descent Directions')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4758fee1-218e-4a56-b696-a00dd9e15f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_with_gradients(lambda v : J(x, y_real, v), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c3ceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "params = np.array([-1.0, 20])  # [a, b]\n",
    "alpha = 0.001   # learning rate\n",
    "iterations = 200\n",
    "\n",
    "cost_history = []\n",
    "for i in range(iterations):\n",
    "    grads = numerical_gradient(lambda v : J(x, y_real, v), params)\n",
    "    params = params - alpha * grads\n",
    "    cost_history.append(J(x, y_real, params))\n",
    "\n",
    "a_gd, b_gd = params\n",
    "print(f\"After GD: a={a_gd:.3f}, b={b_gd:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f738b811",
   "metadata": {},
   "source": [
    "## 5. Visualizing Convergence and Fit\n",
    "Let's plot the cost function over iterations and compare the fitted curve to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28793061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cost history\n",
    "plt.figure()\n",
    "plt.plot(cost_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost J')\n",
    "plt.title('Gradient Descent Convergence')\n",
    "\n",
    "# Plot fitted curve\n",
    "plt.figure()\n",
    "plt.scatter(x, y_real, label='Real data')\n",
    "y_fit_gd = f(x, params)\n",
    "plt.plot(x, y_fit_gd, 'g--', label='GD Fit')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0a946d",
   "metadata": {},
   "source": [
    "## 6. Discussion\n",
    "- You can see how the cost decreases over iterations when the learning rate is appropriate.\n",
    "- Too large a learning rate can cause divergence; too small makes convergence slow.\n",
    "- SciPy's `curve_fit` uses more advanced methods (Levenbergâ€“Marquardt) for faster, more reliable fitting.\n",
    "\n",
    "Feel free to experiment with different learning rates, number of iterations, and models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e36c908-e046-4f4f-ac92-c51dd7c876d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6281f87e-a98e-4c68-a417-9ef767090ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d698dc6f-7435-4c17-85d0-425032349fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2da9d94-f869-4a54-9450-b512bd3715d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
