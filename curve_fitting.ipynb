{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afd6969d",
   "metadata": {},
   "source": [
    "# Curve Fitting via Gradient Descent\n",
    "This notebook walks through fitting a data to a model using **gradient descent**, breaking down the optimization problem step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3ce933-0036-4a84-9895-ad477e4eab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2390c1cf-fcd0-4ebe-b5cf-0809201e5df3",
   "metadata": {},
   "source": [
    "## 1. The principles of curve fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf672664-3536-477c-8052-4867bdcb2b50",
   "metadata": {},
   "source": [
    "Suppose we observe a pendulum in motion. We expect its displacement after time $x$ is $a\\sin(x + b)$, for parameters $a$ and $b$ to be determined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85325d45-f45b-4476-b84f-b381fd6a7866",
   "metadata": {},
   "source": [
    "Mathematically, the displacement function will be $f(x; a, b) = a\\sin(x + b)$. This can be expressed in Python as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c15f09-feb7-4e76-8b55-dffce442e699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, params):\n",
    "    a, b = params\n",
    "    return a * np.sin(x + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05dc9f2-faee-4e63-a4d9-c77482ee6d70",
   "metadata": {},
   "source": [
    "Let's plot this function for $0 \\le x \\le 10$ where $a = 2$ and $b = 1$. That is, we're plotting $f(x, 2, 1)$ for $0 \\le x \\le 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d532bc8b-c735-478b-b3e6-c00b885763d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True parameters\n",
    "a_true, b_true = 2, 1\n",
    "params = (a_true, b_true)\n",
    "x = np.linspace(0, 10, 100)\n",
    "y_true = f(x, params)\n",
    "\n",
    "# Plot data\n",
    "plt.plot(x, y_true, 'r-', label='True function')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5744014-8860-4c6d-a1be-069916b7711f",
   "metadata": {},
   "source": [
    "We can simulate some *noisy* data by adding Gaussian (normally distributed) noise to the true function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ce7609-6062-4d46-adec-ecd1f60aea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_noisy = y_true + 0.1 * np.random.normal(size=x.size)\n",
    "\n",
    "# Plot data\n",
    "plt.scatter(x, y_noisy, label='Noisy data')\n",
    "plt.plot(x, y_true, 'r-', label='True function')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e010ffc9-ca5b-48d3-9dbc-a0f2eead36a0",
   "metadata": {},
   "source": [
    "## 2. Measuring the error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a6b951-6efe-4399-989c-26e39246ced4",
   "metadata": {},
   "source": [
    "What if we *don't* know the parameters $a$ and $b$? If we only have the data, how would we estimate the parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57f05b2-58d5-4703-ab7e-347eafadb6ec",
   "metadata": {},
   "source": [
    "Here is some real data from an observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439bd5bf-8d38-412a-9ec0-fefce1cadc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_real_true, b_real_true = 3.4, 4.2\n",
    "params = (a_real_true, b_real_true)\n",
    "x = np.linspace(0, 10, 100)\n",
    "y_real_true = f(x, params)\n",
    "y_real = y_real_true + 0.01 * np.random.normal(size=x.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd045965-30aa-49dd-abfc-2e7702a05f30",
   "metadata": {},
   "source": [
    "### Activity 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd54031-def0-4554-8d26-45b1298b980b",
   "metadata": {},
   "source": [
    "**Play** with the parameters `a_guess` and `b_guess` belows to try to find a good fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9841696-bbbb-4df9-b779-acac0182cac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guess parameters. Change these!!\n",
    "a_guess, b_guess = 2, 3\n",
    "\n",
    "y_guess = f(x, (a_guess, b_guess))\n",
    "plt.scatter(x, y_real, label='Real data')\n",
    "plt.plot(x, y_guess, 'r-', label='Estimated function')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01ce283",
   "metadata": {},
   "source": [
    "## 2. The Optimization Problem\n",
    "We want to find parameters $(a, b)$ that minimize the **sum of squared errors**:\n",
    "\n",
    "$$\n",
    "J(a, b) = \\frac{1}{2n} \\sum_{i=1}^n (y_i - f(x_i; a, b))^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34ba4f0-1c01-45c2-b8f0-e58d712f7e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(x, y, params):\n",
    "    n = x.size\n",
    "    residuals = y - f(x, params)\n",
    "    return (residuals**2).sum() / (2*n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af209723-171d-4118-8ef4-f917baa272c4",
   "metadata": {},
   "source": [
    "### Activity 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39356872-fb14-4dba-a6c6-81b38969ded7",
   "metadata": {},
   "source": [
    "Play with the values `a_guess` and `b_guess` below to try to minimise $J$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297e94b6-4479-40ee-b1af-6a3acbafb110",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_guess, b_guess = 1, 1\n",
    "\n",
    "J(x,y_real, (a_guess, b_guess))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba4941d",
   "metadata": {},
   "source": [
    "## 3. Contour plots of the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fa27fd-0dbf-4d80-9c31-a1d366d67727",
   "metadata": {},
   "source": [
    "For a fixed data set, our cost function $J$ depends on two parameters $a$ and $b$. We can think of the $J$-values as representing the height of a surface above the $a$-$b$ plane.\n",
    "\n",
    "Our goal is to find the location of the lowest point on this surface: the $(a, b)$ pair that minimises $J$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d8fd13-8c14-4b28-8271-8c9f393e8892",
   "metadata": {},
   "source": [
    "To start, let's visualise the surface using a contour plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865317ae-de75-4e84-a7ce-12798a18604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "def contour_plot(func, amin=-10, amax=10, bmin=-10, bmax=10):\n",
    "    a_vals = np.linspace(amin, amax, 100)\n",
    "    b_vals = np.linspace(bmin, bmax, 100)\n",
    "    A, B = np.meshgrid(a_vals, b_vals)\n",
    "    \n",
    "    # Compute the cost on the grid\n",
    "    Cost = np.empty_like(A)\n",
    "    for i in range(A.shape[0]):\n",
    "        for j in range(A.shape[1]):\n",
    "            Cost[i, j] = func(A[i, j], B[i, j])\n",
    "    \n",
    "    # Plot the contour\n",
    "    plt.figure(figsize=(6,5))\n",
    "    CS = plt.contour(A, B, Cost, levels=15)\n",
    "    plt.clabel(CS, inline=True, fontsize=8)\n",
    "    plt.xlabel('a parameter')\n",
    "    plt.ylabel('b parameter')\n",
    "    plt.title('Contour Plot')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ab4779-2ef7-4c11-a606-fd6b520c7c23",
   "metadata": {},
   "source": [
    "### Activity 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e57867e-bae8-472b-9b38-17ee48dc5bd9",
   "metadata": {},
   "source": [
    "The plot below shows contours of constant $J$. We're trying to find the lowest value. Estimate the optimal $(a, b)$ pair from the plot. Feel free to change the plot area by altering `amin`, `amax`, `bmin` and `bmax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28f5dde-df48-440e-9a61-ac6b18ac49a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_plot(lambda a, b : J(x, y_real, (a, b)), amin=-6, amax=6, bmin=-6, bmax=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bc62a0-f4a5-4ded-ac09-4a684bb1563b",
   "metadata": {},
   "source": [
    "## 4. Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a9e050-819d-465c-a2fa-4b49ff8694a6",
   "metadata": {},
   "source": [
    "Gradient descent is a general method for numerically finding the minimal value of a function of many variables.\n",
    "\n",
    "**Idea** if you're standing on a hill and want to find the top of the hill, walk in the steepest upward direction. To find the bottom of the hill, walk in the steepest downward direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afceebd-1c58-42a4-87d5-f87f2bb3a70e",
   "metadata": {},
   "source": [
    "It turns out that the *gradient vector* gives this direction of steepest ascent. In our case, this is the vector\n",
    "$$\n",
    "\\nabla J = \\left(\\frac{\\partial J}{\\partial a},\\frac{\\partial J}{\\partial b}\\right),\n",
    "$$\n",
    "where $\\partial J/ \\partial a$ is the *partial derivative* of $J$ with respect to $a$.\n",
    "\n",
    "Evaluated at a point $(a_0, b_0)$, this partial derivative is the limit, as $h$ tends to $0$, of\n",
    "$$\n",
    "\\frac{J(a_0 + h, b_0) - J(a_0, b_0)}{h},\n",
    "$$\n",
    "much like an ordinary dervative. Likewise, the $\\partial J/ \\partial b$, evaluated at $(a_0, b_0)$ is the limit as $h$ tends to $0$ of\n",
    "$$\n",
    "\\frac{J(a_0, b_0 + h) - J(a_0, b_0)}{h}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bb8161-12fc-4b14-81f7-02fb6b5a92b0",
   "metadata": {},
   "source": [
    "Here's a function that numerically estimates the gradient vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be0cfc6-2432-4a76-bb34-ad147e2794e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(func, params, h=1e-5):\n",
    "    \"\"\"\n",
    "    Numerically estimates the gradient of func at params using central differences.\n",
    "    \"\"\"\n",
    "    params = np.array(params, dtype=float)\n",
    "    grad = np.zeros_like(params)\n",
    "\n",
    "    for i in range(len(params)):\n",
    "        step = np.zeros_like(params)\n",
    "        step[i] = h\n",
    "        grad[i] = (func(params + step) - func(params - step)) / (2 * h)\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517ec7fb-0f51-438e-ae06-f2f808a66bef",
   "metadata": {},
   "source": [
    "Using this function, we'll plot the gradient vectors as arrows over our contour plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f926470-9058-4e82-a42a-84decd352184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contour_with_gradients(func, amin=-10, amax=10, bmin=-10, bmax=10):\n",
    "    a_vals, b_vals = np.linspace(amin, amax, 100), np.linspace(bmin, bmax, 100)\n",
    "    size = min(amax - amin, bmax - bmin)\n",
    "    A, B = np.meshgrid(a_vals, b_vals)\n",
    "    \n",
    "    Cost = np.empty_like(A)\n",
    "    for i in range(A.shape[0]):\n",
    "        for j in range(A.shape[1]):\n",
    "            Cost[i, j] = func((A[i, j], B[i, j]))\n",
    "\n",
    "    # Compute gradients\n",
    "    step = 8\n",
    "    A_s, B_s = A[::step, ::step], B[::step, ::step]\n",
    "    dA, dB = np.zeros_like(A_s), np.zeros_like(B_s)\n",
    "    for i in range(A_s.shape[0]):\n",
    "        for j in range(A_s.shape[1]):\n",
    "            dA[i,j], dB[i,j] = -numerical_gradient(func,(A_s[i,j], B_s[i,j]))\n",
    "    \n",
    "    norm = np.linalg.norm(np.array((dA, dB)), axis=0)\n",
    "    dA, dB = dA / norm, dB / norm\n",
    "    \n",
    "    # Plot contour + quiver (gradient arrows)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    CS = plt.contour(A, B, Cost, levels=20, cmap = 'cividis')\n",
    "    plt.clabel(CS, inline=True, fontsize=8)\n",
    "    plt.quiver(A_s, B_s, dA, dB, \n",
    "               angles='xy', scale_units='xy', scale=30/size, width=0.003, color='red')\n",
    "    plt.xlabel('a parameter')\n",
    "    plt.ylabel('b parameter')\n",
    "    plt.title('Contours with Gradient Descent Directions')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059ad8c7-190e-401a-bf98-fdc58db0c319",
   "metadata": {},
   "source": [
    "### Activity 4\n",
    "Run the code below to produce the contour plot with gradient vectors. What is the relationship between the contour lines and the direction of the gradient vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4758fee1-218e-4a56-b696-a00dd9e15f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_with_gradients(lambda v : J(x, y_real, v), amin=-6, amax=6, bmin=-6, bmax=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5623b0f",
   "metadata": {},
   "source": [
    "## 5. Implementing Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a18615c-4c3b-44d2-acbc-1066627e8244",
   "metadata": {},
   "source": [
    "To find the minimum value of $J$ using gradient descent, we'll begin with a starting choice $(a_0, b_0)$ of parameters and iteratively compute:\n",
    "$$\n",
    "(a_{i+1}, b_{i+1}) = (a_i, b_i) + \\alpha \\nabla J,\n",
    "$$\n",
    "for a small value $\\alpha$. This has the effect of taking a small step from point $(a_i, b_i)$ in the direction of the gradient vector $\\nabla J$.\n",
    "\n",
    "Gradient descent is heavily used in training machine learning models where the value $\\alpha$ is called the *learning rate*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe8e703-c2b8-445d-8037-697c19f9af51",
   "metadata": {},
   "source": [
    "### Activity 5\n",
    "\n",
    "Run the code below. Try a few different values of the learning rate $\\alpha$ and number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c3ceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "params = np.array([5.0, 1.0])  # [a, b]\n",
    "alpha = 3.2   # learning rate\n",
    "iterations = 50 # number of iterations\n",
    "\n",
    "cost_history = []\n",
    "for i in range(iterations):\n",
    "    grads = numerical_gradient(lambda v : J(x, y_real, v), params)\n",
    "    params = params - alpha * grads\n",
    "    cost_history.append(J(x, y_real, params))\n",
    "\n",
    "a_gd, b_gd = params\n",
    "print(f\"After GD: a={a_gd:.3f}, b={b_gd:.3f}, J={J(x, y_real, params):.3e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f738b811",
   "metadata": {},
   "source": [
    "## 6. Visualizing Convergence and Fit\n",
    "Let's plot the cost function over iterations and compare the fitted curve to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28793061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cost history\n",
    "plt.figure()\n",
    "plt.plot(cost_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost J')\n",
    "plt.title('Gradient Descent Convergence')\n",
    "\n",
    "# Plot fitted curve\n",
    "plt.figure()\n",
    "plt.scatter(x, y_real, label='Real data')\n",
    "y_fit_gd = f(x, params)\n",
    "plt.plot(x, y_fit_gd, 'g--', label='GD Fit')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0a946d",
   "metadata": {},
   "source": [
    "## 7. Discussion\n",
    "- You can see how the cost decreases over iterations when the learning rate is appropriate.\n",
    "- Too large a learning rate can cause divergence; too small makes convergence slow.\n",
    "- SciPy's `curve_fit` uses more advanced methods (Levenbergâ€“Marquardt) for faster, more reliable fitting.\n",
    "\n",
    "Feel free to experiment with different learning rates, number of iterations, and models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e36c908-e046-4f4f-ac92-c51dd7c876d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6281f87e-a98e-4c68-a417-9ef767090ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d698dc6f-7435-4c17-85d0-425032349fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2da9d94-f869-4a54-9450-b512bd3715d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
