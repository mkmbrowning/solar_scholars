{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afd6969d",
   "metadata": {},
   "source": [
    "# Curve Fitting via Gradient Descent\n",
    "This notebook walks through fitting a data to a model using **gradient descent**, breaking down the optimization problem step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3ce933-0036-4a84-9895-ad477e4eab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2390c1cf-fcd0-4ebe-b5cf-0809201e5df3",
   "metadata": {},
   "source": [
    "## 1. The principles of curve fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf672664-3536-477c-8052-4867bdcb2b50",
   "metadata": {},
   "source": [
    "Suppose we observe the flight of a cannon ball. It *should* be a parabola (a quadratic function), but our observations will have some noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c15f09-feb7-4e76-8b55-dffce442e699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, a, b):\n",
    "    return a * x*x + b * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05dc9f2-faee-4e63-a4d9-c77482ee6d70",
   "metadata": {},
   "source": [
    "Plotting the true curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d532bc8b-c735-478b-b3e6-c00b885763d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True parameters\n",
    "a_true, b_true = -2, 20\n",
    "x = np.linspace(0, 10, 100)\n",
    "y_true = f(x, a_true, b_true)\n",
    "\n",
    "# Plot data\n",
    "plt.plot(x, y_true, 'r-', label='True function')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5744014-8860-4c6d-a1be-069916b7711f",
   "metadata": {},
   "source": [
    "We can simulate some *noisy* data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ce7609-6062-4d46-adec-ecd1f60aea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_noisy = y_true + 2 * np.random.normal(size=x.size)\n",
    "\n",
    "# Plot data\n",
    "plt.scatter(x, y_noisy, label='Noisy data')\n",
    "plt.plot(x, y_true, 'r-', label='True function')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e010ffc9-ca5b-48d3-9dbc-a0f2eead36a0",
   "metadata": {},
   "source": [
    "## 2. Measuring the error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a6b951-6efe-4399-989c-26e39246ced4",
   "metadata": {},
   "source": [
    "What if we *don't* know the parameters $a$, $b$, and $c$? If we only have the data, how would we estimate the parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57f05b2-58d5-4703-ab7e-347eafadb6ec",
   "metadata": {},
   "source": [
    "Here is some real data from an observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13416fff-d53e-4b8d-ac86-627819c0ac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_real = np.array([ 2.36940577,  1.91256656,  3.29100004,  7.37237899,  8.18812265,\n",
    "       18.11820761, 14.32964108, 23.45234193, 23.62329705, 30.8287573 ,\n",
    "       26.80806497, 32.79398541, 35.06553443, 32.21923115, 39.24730387,\n",
    "       42.92070059, 37.44160446, 46.44210828, 47.98489458, 50.6389694 ,\n",
    "       53.82037364, 51.7647055 , 56.38908139, 56.26206115, 61.48547156,\n",
    "       59.12983318, 61.26146892, 61.38177205, 61.7767669 , 67.62397392,\n",
    "       69.1165745 , 68.00363628, 68.06124047, 66.05407344, 74.01905039,\n",
    "       72.48996972, 77.38943085, 74.95610992, 71.69048994, 76.73277233,\n",
    "       75.61839857, 75.18028164, 80.42250882, 78.57339426, 74.52704359,\n",
    "       74.43486386, 82.60185162, 80.63046288, 79.27165332, 83.2890151 ,\n",
    "       78.7393532 , 77.31638276, 76.05766149, 79.93963762, 85.7438112 ,\n",
    "       78.3924083 , 75.06743955, 73.37798833, 80.24202993, 77.18560786,\n",
    "       78.01280817, 78.06876329, 73.34550463, 72.32834926, 75.91838852,\n",
    "       76.06033239, 73.46894369, 70.89037719, 69.35203876, 69.2537387 ,\n",
    "       64.88669121, 61.9744727 , 66.14014532, 61.41478708, 62.08014846,\n",
    "       59.97290679, 55.71001107, 55.72145555, 55.60052885, 51.0761939 ,\n",
    "       48.45070483, 46.66118886, 45.4664427 , 49.08572602, 41.67735713,\n",
    "       42.42097927, 37.24459493, 33.91618618, 29.18840866, 27.44201465,\n",
    "       28.78153111, 27.63522533, 21.24672748, 18.50438048, 10.3768495 ,\n",
    "       11.26647595, 11.80014487,  5.91476614,  1.61126693,  1.5332323 ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd045965-30aa-49dd-abfc-2e7702a05f30",
   "metadata": {},
   "source": [
    "### Activity 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff0f3ac-ca6b-4d07-b194-7d2191bcf17d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7dd54031-def0-4554-8d26-45b1298b980b",
   "metadata": {},
   "source": [
    "**Play** with the parameters `a_guess` and `b_guess` belows to try to find a good fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9841696-bbbb-4df9-b779-acac0182cac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guess parameters. Change these!!\n",
    "a_guess, b_guess = -1, 10\n",
    "y_guess = f(x, a_guess, b_guess)\n",
    "\n",
    "\n",
    "plt.scatter(x, y_real, label='Real data')\n",
    "plt.plot(x, y_guess, 'r-', label='Estimated function')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01ce283",
   "metadata": {},
   "source": [
    "## 2. The Optimization Problem\n",
    "We want to find parameters $(a, b)$ that minimize the **sum of squared errors**:\n",
    "\n",
    "$$\n",
    "J(a, b) = \\frac{1}{2n} \\sum_{i=1}^n (y_i - f(x_i; a, b))^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34ba4f0-1c01-45c2-b8f0-e58d712f7e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(x, y, params):\n",
    "    a, b = params\n",
    "    n = x.size\n",
    "    residuals = y - f(x, a, b)\n",
    "    return (residuals**2).sum() / (2*n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af209723-171d-4118-8ef4-f917baa272c4",
   "metadata": {},
   "source": [
    "### Activity 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39356872-fb14-4dba-a6c6-81b38969ded7",
   "metadata": {},
   "source": [
    "Play with the values `a_guess` and `b_guess` below to try to minimise $J$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297e94b6-4479-40ee-b1af-6a3acbafb110",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_guess, b_guess = -1, 10\n",
    "J(x,y_real, (a_guess, b_guess))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba4941d",
   "metadata": {},
   "source": [
    "## 3. Gradient Descent\n",
    "Gradient descent is a general method for numerically finding the minimal value of a function of many variables.\n",
    "\n",
    "**Idea** if you're standing on a hill and want to find the bottom of the hill, walk in the steepest downward direction.\n",
    "\n",
    "iteratively updates parameters in the direction that **reduces** the cost. The update rule for each parameter $\\theta_j$ is:\n",
    "\n",
    "$$\n",
    "\\theta_j \\leftarrow \\theta_j - \\alpha \\frac{\\partial J}{\\partial \\theta_j},\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\alpha$ is the **learning rate** (step size).\n",
    "- $\\frac{\\partial J}{\\partial \\theta_j}$ is the gradient of the cost w.r.t. $\\theta_j$.\n",
    "\n",
    "For our model $f(x; a,b) = ax^2 + bx$, the partial derivatives are:\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial a} = \\frac{1}{n} \\sum (y_i - f_i) x^2,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{1}{n} \\sum (y_i - f_i) x.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865317ae-de75-4e84-a7ce-12798a18604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_vals = np.linspace(-4, 0, 100)   # adjust range as needed\n",
    "b_vals = np.linspace(0, 40, 100)   # adjust range as needed\n",
    "A, B = np.meshgrid(a_vals, b_vals)\n",
    "\n",
    "# 4) Compute the cost on the grid\n",
    "Cost = np.empty_like(A)\n",
    "for i in range(A.shape[0]):\n",
    "    for j in range(A.shape[1]):\n",
    "        Cost[i, j] = J(x, y_true, (A[i, j], B[i, j]))\n",
    "\n",
    "# 5) Plot the contour\n",
    "plt.figure(figsize=(6,5))\n",
    "CS = plt.contour(A, B, Cost, levels=15)\n",
    "plt.clabel(CS, inline=True, fontsize=8)\n",
    "plt.xlabel('a parameter')\n",
    "plt.ylabel('b parameter')\n",
    "plt.title('Contour Plot of Cost(a, b)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bab91fc-8457-4346-ba33-83d3e7a67901",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 8   # pick every 8th point ⇒ 100/8≈12 arrows each axis\n",
    "A_s = A[::step, ::step]\n",
    "B_s = B[::step, ::step]\n",
    "dA = np.zeros_like(A_s)\n",
    "dB = np.zeros_like(B_s)\n",
    "for i in range(A_s.shape[0]):\n",
    "    for j in range(A_s.shape[1]):\n",
    "        grad = compute_gradient(x, y_true, A_s[i,j], B_s[i,j])\n",
    "        # we want arrows pointing downhill, so negate the gradient:\n",
    "        dA[i,j], dB[i,j] = -grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f4999d-2535-4808-a3f5-9eaf8ce38a10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5623b0f",
   "metadata": {},
   "source": [
    "## 4. Implementing Gradient Descent\n",
    "Let's code gradient descent to fit the parameters. We'll track the cost over iterations and inspect convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c541860-ef68-43ca-a1ab-064ca9618b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x, y, params):\n",
    "    a, b = params\n",
    "    m = x.size\n",
    "    y_pred = f(x, a, b)\n",
    "    residuals = y - y_pred\n",
    "    # gradients\n",
    "    da = -np.sum(residuals * (x*x)) / m\n",
    "    db = -np.sum(residuals * x) / m\n",
    "    return np.array([da, db])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2952090d-d758-4cae-852a-a4d256aa54f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_gradient(x, y_true, (-1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c3ceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "params = np.array([-1.0, 20])  # [a, b]\n",
    "alpha = 0.0001   # learning rate\n",
    "iterations = 200\n",
    "\n",
    "cost_history = []\n",
    "for i in range(iterations):\n",
    "    grads = compute_gradient(x, y_real, params)\n",
    "    params = params - alpha * grads\n",
    "    cost_history.append(compute_cost(x, y_noisy, params))\n",
    "\n",
    "a_gd, b_gd = params\n",
    "print(f\"After GD: a={a_gd:.3f}, b={b_gd:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f738b811",
   "metadata": {},
   "source": [
    "## 5. Visualizing Convergence and Fit\n",
    "Let's plot the cost function over iterations and compare the fitted curve to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28793061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cost history\n",
    "plt.figure()\n",
    "plt.plot(cost_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost J')\n",
    "plt.title('Gradient Descent Convergence')\n",
    "\n",
    "# Plot fitted curve\n",
    "plt.figure()\n",
    "plt.scatter(x, y_real, label='Noisy data')\n",
    "y_fit_gd = f(x, a_gd, b_gd)\n",
    "plt.plot(x, y_fit_gd, 'g--', label='GD Fit')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0a946d",
   "metadata": {},
   "source": [
    "## 6. Discussion\n",
    "- You can see how the cost decreases over iterations when the learning rate is appropriate.\n",
    "- Too large a learning rate can cause divergence; too small makes convergence slow.\n",
    "- SciPy's `curve_fit` uses more advanced methods (Levenberg–Marquardt) for faster, more reliable fitting.\n",
    "\n",
    "Feel free to experiment with different learning rates, number of iterations, and models!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
